{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c728dbc0-3bb7-4370-9110-f7fcc6b8af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-core langchain-Ollama langchain_community -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7bd79-2b44-449c-bfee-89650de6bb9f",
   "metadata": {},
   "source": [
    "## Tools and Agents using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2d5d2b2-713e-4c76-8076-80136be087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38cde5d9-3111-4745-93d6-26c1b6b49062",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Based on user query, look for information using DuckDuckGo Search and Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bf96c1f-25c9-46bf-b5fe-bfe4b8c587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc07d2f2-c79c-4d2a-a1c9-8768921ad792",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [search, wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576f9b82-b475-48ac-997e-2bfe91d06688",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab70ae1-7a3f-4d1d-bdb8-479d16053219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_results_json` with `{'query': 'Ollama running LLM locally'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[snippet: 2. Running Models. To interact with your locally hosted LLM, you can use the command line directly or via an API. For command-line interaction, Ollama provides the `ollama run <name-of-model ..., title: Ollama and LangChain: Run LLMs locally - Medium, link: https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46], [snippet: How to Run Ollama. To show you the power of using open source LLMs locally, I'll present multiple examples with different open source models with different use-cases. This will help you to use any future open source LLM models with ease. So, lets get started with the first example! How to Run the LLama2 Model from Meta, title: How to Run Open Source LLMs Locally Using Ollama - freeCodeCamp.org, link: https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/], [snippet: Ollama sets itself up as a local server on port 11434. We can do a quick curl command to check that the API is responding. Here is a non-streaming (that is, not interactive) REST call via Warp with a JSON style payload: The response was: \"response\": \"nThe sky appears blue because of a phenomenon called Rayleigh..\", title: How to Set up and Run a Local LLM with Ollama and Llama 2, link: https://thenewstack.io/how-to-set-up-and-run-a-local-llm-with-ollama-and-llama-2/], [snippet: Jun 3, 2024. 7. As part of the LLM deployment series, this article focuses on implementing Llama 3 with Ollama. Ollama is a powerful tool that allows users to run open-source large language models ..., title: Run Llama 3 Locally with Ollama - Medium, link: https://medium.com/@manuelescobar-dev/running-llama-3-locally-with-ollama-9881706df7ac]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the search results, it appears that Ollama can be used to run Large Language Models (LLMs) locally. To do this, you can use the `ollama run` command followed by the name of the model you want to interact with.\n",
      "\n",
      "For example, if you want to run the LLama2 Model from Meta, you can use the following command:\n",
      "\n",
      "```\n",
      "ollama run <name-of-model>\n",
      "```\n",
      "\n",
      "You can also use Ollama to set up a local server on port 11434 and make REST calls via Warp with a JSON style payload.\n",
      "\n",
      "Additionally, there are resources available online that provide step-by-step guides on how to set up and run LLMs locally using Ollama and Llama 2.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is Ollama used for running LLM locally',\n",
       " 'output': 'Based on the search results, it appears that Ollama can be used to run Large Language Models (LLMs) locally. To do this, you can use the `ollama run` command followed by the name of the model you want to interact with.\\n\\nFor example, if you want to run the LLama2 Model from Meta, you can use the following command:\\n\\n```\\nollama run <name-of-model>\\n```\\n\\nYou can also use Ollama to set up a local server on port 11434 and make REST calls via Warp with a JSON style payload.\\n\\nAdditionally, there are resources available online that provide step-by-step guides on how to set up and run LLMs locally using Ollama and Llama 2.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"How is Ollama used for running LLM locally\"})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "161807c3-c5c2-4d88-a733-e21fb5f38300",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "211d1a0a-0820-4393-bdf6-d062b2930499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Yann LeCun'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Yann LeCun\n",
      "Summary: Yann André LeCun ( lə-KUN, French: [ləkœ̃]; originally spelled Le Cun; born 8 July 1960) is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics and computational neuroscience. He is the Silver Professor of the Courant Institute of Mathematical Sciences at New York University and Vice-President, Chief AI Scientist at Meta.\n",
      "He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). He is also one of the main creators of the DjVu image compression technology (together with Léon Bottou and Patrick Haffner). He co-developed the Lush programming language with Léon Bottou.\n",
      "In 2018, LeCun, Yoshua Bengio, and Geoffrey Hinton, received the Turing Award for their work on deep learning. The three are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\".\n",
      "\n",
      "\n",
      "\n",
      "Page: LeNet\n",
      "Summary: LeNet is a series of convolutional neural network structure proposed by LeCun et al.. The earliest version, LeNet-1, was trained in 1989. In general, when \"LeNet\" is referred to without a number, it refers to LeNet-5 (1998), the most well-known version.\n",
      "Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing. LeNet-5 was one of the earliest convolutional neural networks and was historically important during the development of deep learning.\n",
      "\n",
      "Page: Geoffrey Hinton\n",
      "Summary: Geoffrey Everest Hinton  (born 6 December 1947) is a British-Canadian  computer scientist and cognitive psychologist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.\n",
      "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.\n",
      "Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together.\n",
      "In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.I.\" He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence. He noted that establishing safety guidelines will require cooperation among those competing in use of AI in order to avoid the worst outcomes.\u001b[0m\u001b[32;1m\u001b[1;3mYann LeCun is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun co-developed the Lush programming language with Léon Bottou and co-created the DjVu image compression technology. He also co-developed the LeNet series of convolutional neural network structures. In 2018, LeCun received the Turing Award for his work on deep learning, along with Yoshua Bengio and Geoffrey Hinton.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is Yann LeCun',\n",
       " 'output': 'Yann LeCun is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun co-developed the Lush programming language with Léon Bottou and co-created the DjVu image compression technology. He also co-developed the LeNet series of convolutional neural network structures. In 2018, LeCun received the Turing Award for his work on deep learning, along with Yoshua Bengio and Geoffrey Hinton.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"Who is Yann LeCun\"})\n",
    "\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
