{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c728dbc0-3bb7-4370-9110-f7fcc6b8af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-core langchain-Ollama langchain_community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckduckgo-search in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (7.2.0)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=8.1.7 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: primp>=0.9.3 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from duckduckgo-search) (0.9.3)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from duckduckgo-search) (5.3.0)\n",
      "Collecting beautifulsoup4 (from wikipedia)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=76c4ba0dda9c90c25decd4c1763d33ada1dccd99920423533f485483c3dfa69d\n",
      "  Stored in directory: /Users/andrestrepo/Library/Caches/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U duckduckgo-search wikipedia -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7bd79-2b44-449c-bfee-89650de6bb9f",
   "metadata": {},
   "source": [
    "## Tools and Agents using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d5d2b2-713e-4c76-8076-80136be087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cde5d9-3111-4745-93d6-26c1b6b49062",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Based on user query, look for information using DuckDuckGo Search and Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf96c1f-25c9-46bf-b5fe-bfe4b8c587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8B\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc07d2f2-c79c-4d2a-a1c9-8768921ad792",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [search, wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576f9b82-b475-48ac-997e-2bfe91d06688",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ab70ae1-7a3f-4d1d-bdb8-479d16053219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_results_json` with `{'query': 'Ollama LLM local run'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrestrepo/Documents/repos_personal/ollama/.venv/lib/python3.12/site-packages/langchain_community/utilities/duckduckgo_search.py:64: UserWarning: backend='api' is deprecated, using backend='auto'\n",
      "  ddgs_gen = ddgs.text(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3msnippet: Estimated reading time: 5 minutes Introduction This guide will show you how to easily set up and run large language models (LLMs) locally using Ollama and Open WebUI on Windows, Linux, or macOS - without the need for Docker. Ollama provides local model inference, and Open WebUI is a user interface that simplifies interacting with these models., title: Ollama + Open WebUI: A Way to Run LLMs Locally, link: https://www.itflashcards.com/blog/ollama-open-webui-run-llms-locally/, snippet: 2. Running Models. To interact with your locally hosted LLM, you can use the command line directly or via an API. For command-line interaction, Ollama provides the `ollama run <name-of-model ..., title: Ollama and LangChain: Run LLMs locally - Medium, link: https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46, snippet: Just fire up ollama serve to run Ollama without the desktop app. I've also created a simple React app using Ollama's REST API for straightforward LLM interactions. Check out the code on GitHub ..., title: Running LLM Locally: A Beginner's Guide to Using Ollama, link: https://medium.com/@arunpatidar26/run-llm-locally-ollama-8ea296747505, snippet: Enter Ollama, a platform that makes local development with open-source large language models a breeze. With Ollama, everything you need to run an LLM—model weights and all of the config—is packaged into a single Modelfile. Think Docker for LLMs. In this tutorial, we'll take a look at how to get started with Ollama to run large language ..., title: Ollama Tutorial: Running LLMs Locally Made Super Simple, link: https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple\u001b[0m\u001b[32;1m\u001b[1;3mYou can use Ollama for running Large Language Models (LLMs) locally by following these steps:\n",
      "\n",
      "1. Install Ollama on your system.\n",
      "2. Create a Modelfile that includes the model weights and configuration.\n",
      "3. Use the `ollama run` command to interact with the LLM.\n",
      "\n",
      "Alternatively, you can use Open WebUI as a user interface to simplify interacting with the LLM.\n",
      "\n",
      "You can also use Ollama's REST API for straightforward LLM interactions.\n",
      "\n",
      "For more information, you can refer to the following resources:\n",
      "\n",
      "* https://www.itflashcards.com/blog/ollama-open-webui-run-llms-locally/\n",
      "* https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46\n",
      "* https://medium.com/@arunpatidar26/run-llm-locally-ollama-8ea296747505\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is Ollama used for running LLM locally',\n",
       " 'output': \"You can use Ollama for running Large Language Models (LLMs) locally by following these steps:\\n\\n1. Install Ollama on your system.\\n2. Create a Modelfile that includes the model weights and configuration.\\n3. Use the `ollama run` command to interact with the LLM.\\n\\nAlternatively, you can use Open WebUI as a user interface to simplify interacting with the LLM.\\n\\nYou can also use Ollama's REST API for straightforward LLM interactions.\\n\\nFor more information, you can refer to the following resources:\\n\\n* https://www.itflashcards.com/blog/ollama-open-webui-run-llms-locally/\\n* https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46\\n* https://medium.com/@arunpatidar26/run-llm-locally-ollama-8ea296747505\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"How is Ollama used for running LLM locally\"})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "161807c3-c5c2-4d88-a733-e21fb5f38300",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211d1a0a-0820-4393-bdf6-d062b2930499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Yann LeCun'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Yann LeCun\n",
      "Summary: Yann André LeCun ( lə-KUN, French: [ləkœ̃]; originally spelled Le Cun; born 8 July 1960) is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics and computational neuroscience. He is the Silver Professor of the Courant Institute of Mathematical Sciences at New York University and Vice President, Chief AI Scientist at Meta.\n",
      "He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). He is also one of the main creators of the DjVu image compression technology (together with Léon Bottou and Patrick Haffner). He co-developed the Lush programming language with Léon Bottou.\n",
      "In 2018, LeCun, Yoshua Bengio, and Geoffrey Hinton, received the Turing Award for their work on deep learning. The three are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\".\n",
      "\n",
      "Page: Geoffrey Hinton\n",
      "Summary: Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian computer scientist, cognitive scientist, cognitive psychologist, known for his work on artificial neural networks which earned him the title as the \"Godfather of AI\".\n",
      "Hinton is University Professor Emeritus at the University of Toronto. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023, citing concerns about the many risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.\n",
      "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularised the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.\n",
      "Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together. He was also awarded with John Hopfield the 2024 Nobel Prize in Physics for foundational discoveries and inventions that enable machine learning with artificial neural networks.\n",
      "In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.I.\" He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence. He noted that establishing safety guidelines will require cooperation among those competing in use of AI in order to avoid the worst outcomes. After receiving the Nobel Prize, he called for urgent research into AI safety to figure out how to control AI systems smarter than humans.\n",
      "\n",
      "Page: LeNet\n",
      "Summary: LeNet is a series of convolutional neural network structure proposed by LeCun et al.. The earliest version, LeNet-1, was trained in 1989. In general, when \"LeNet\" is referred to without a number, it refers to LeNet-5 (1998), the most well-known version.\n",
      "Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing. LeNet-5 was one of the earliest convolutional neural networks and was historically important during the development of deep learning.\u001b[0m\u001b[32;1m\u001b[1;3mYann LeCun is a French-American computer scientist who has made significant contributions to the field of artificial intelligence, particularly in machine learning, computer vision, and neural networks. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun is also one of the main creators of the DjVu image compression technology and co-developed the Lush programming language. In 2018, he received the Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Geoffrey Hinton, for their work on deep learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is Yann LeCun',\n",
       " 'output': 'Yann LeCun is a French-American computer scientist who has made significant contributions to the field of artificial intelligence, particularly in machine learning, computer vision, and neural networks. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun is also one of the main creators of the DjVu image compression technology and co-developed the Lush programming language. In 2018, he received the Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Geoffrey Hinton, for their work on deep learning.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"Who is Yann LeCun\"})\n",
    "\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
